{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Embed SLAM Models Comparison\n",
                "\n",
                "This notebook compares the models from `embed_slam`: ConceptFusion, DINOFusion, XFusion, and NARadioFusion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "from vlm_eval.core import EncoderRegistry\n",
                "from vlm_eval.encoders import *\n",
                "\n",
                "# Ensure models are registered\n",
                "print(\"Available encoders:\", EncoderRegistry.list_available())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Models\n",
                "# Note: You need to have the necessary checkpoints and dependencies installed.\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "models = {}\n",
                "model_names = [\"concept_fusion\", \"dino_fusion\", \"x_fusion\", \"naradio_fusion\"]\n",
                "\n",
                "for name in model_names:\n",
                "    try:\n",
                "        print(f\"Loading {name}...\")\n",
                "        models[name] = EncoderRegistry.get(name, device=device)\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {name}: {e}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a sample image\n",
                "# Replace with your image path\n",
                "image_path = \"../examples/example_office.jpg\"\n",
                "if not os.path.exists(image_path):\n",
                "    # Create a dummy image if not exists\n",
                "    dummy_img = Image.fromarray(np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8))\n",
                "    dummy_img.save(image_path)\n",
                "    print(f\"Created dummy image at {image_path}\")\n",
                "\n",
                "image = Image.open(image_path).convert(\"RGB\")\n",
                "plt.imshow(image)\n",
                "plt.title(\"Input Image\")\n",
                "plt.show()\n",
                "\n",
                "# Preprocess\n",
                "image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
                "image_tensor = image_tensor.unsqueeze(0).to(device)\n",
                "print(\"Image tensor shape:\", image_tensor.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Inference and Compare\n",
                "text_query = \"chair\"\n",
                "\n",
                "fig, axes = plt.subplots(1, len(models), figsize=(20, 5))\n",
                "if len(models) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for i, (name, model) in enumerate(models.items()):\n",
                "    print(f\"Running {name}...\")\n",
                "    with torch.no_grad():\n",
                "        # Get image features\n",
                "        features = model(image_tensor) # (B, C, H, W)\n",
                "        \n",
                "        # Get text features\n",
                "        text_emb = model.encode_text([text_query]) # (1, C)\n",
                "        \n",
                "        # Compute similarity\n",
                "        # features: (1, C, H, W)\n",
                "        # text_emb: (1, C)\n",
                "        sim = torch.einsum(\"bchw,bc->bhw\", features, text_emb)\n",
                "        \n",
                "        sim_map = sim[0].cpu().numpy()\n",
                "        \n",
                "        axes[i].imshow(sim_map, cmap=\"jet\")\n",
                "        axes[i].set_title(f\"{name} - '{text_query}'\")\n",
                "        axes[i].axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42aa5640",
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "VALID_RESOLUTIONS = [\n",
                "    (192, 192), (224, 224), (256, 256), (288, 288), (320, 320), (384, 384),\n",
                "    (448, 448), (512, 512), (640, 640), (768, 768), (896, 896), (1024, 1024),\n",
                "    (256, 384), (384, 256), (336, 448), (448, 336), (512, 768), (768, 512),\n",
                "    (512, 896), (896, 512), (720, 1280), (800, 1280), (768, 1024), (1024, 768),\n",
                "]\n",
                "\n",
                "def is_naradio_like(enc):\n",
                "    return hasattr(getattr(enc, \"model\", None), \"naradio\") and hasattr(enc.model.naradio, \"input_resolution\")\n",
                "\n",
                "def pick_best_resolution(hw, candidates):\n",
                "    h, w = hw\n",
                "    ar = w / h\n",
                "    best = None\n",
                "    best_score = float(\"inf\")\n",
                "    for H, W in candidates:\n",
                "        ar2 = W / H\n",
                "        score = abs(math.log(ar2 / ar)) + 0.15 * abs(math.log((H * W) / (h * w)))\n",
                "        if score < best_score:\n",
                "            best_score = score\n",
                "            best = (H, W)\n",
                "    return best\n",
                "\n",
                "def letterbox_to(x, target_hw):\n",
                "    B, C, H, W = x.shape\n",
                "    th, tw = target_hw\n",
                "    scale = min(tw / W, th / H)\n",
                "    nh = int(round(H * scale))\n",
                "    nw = int(round(W * scale))\n",
                "    x = F.interpolate(x, size=(nh, nw), mode=\"bilinear\", align_corners=False)\n",
                "\n",
                "    pad_h = th - nh\n",
                "    pad_w = tw - nw\n",
                "    top = pad_h // 2\n",
                "    bottom = pad_h - top\n",
                "    left = pad_w // 2\n",
                "    right = pad_w - left\n",
                "    return F.pad(x, (left, right, top, bottom), value=0.0)\n",
                "\n",
                "def ensure_bchw(feats):\n",
                "    # Accept (B,C,H,W). Anything else -> error.\n",
                "    if not isinstance(feats, torch.Tensor):\n",
                "        raise TypeError(f\"Model returned {type(feats)} not a Tensor\")\n",
                "    if feats.ndim != 4:\n",
                "        raise ValueError(f\"Expected features (B,C,H,W) but got shape {tuple(feats.shape)}\")\n",
                "    return feats\n",
                "\n",
                "text_query = \"chair\"\n",
                "\n",
                "# run models, collect successful ones for plotting\n",
                "ok = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Running {name}...\")\n",
                "    try:\n",
                "        if is_naradio_like(model):\n",
                "            target_hw = pick_best_resolution(image_tensor.shape[-2:], VALID_RESOLUTIONS)\n",
                "            img_in = letterbox_to(image_tensor, target_hw)\n",
                "            print(f\"  -> resized to {target_hw}\")\n",
                "        else:\n",
                "            img_in = image_tensor\n",
                "\n",
                "        with torch.no_grad():\n",
                "            feats = ensure_bchw(model(img_in))\n",
                "            txt = model.encode_text([text_query])\n",
                "            feats = F.normalize(feats, dim=1)\n",
                "            txt = F.normalize(txt, dim=1)\n",
                "            sim = torch.einsum(\"bchw,bc->bhw\", feats, txt)[0].cpu().numpy()\n",
                "\n",
                "        ok.append((name, sim))\n",
                "    except Exception as e:\n",
                "        print(f\"  !! skipping {name}: {type(e).__name__}: {e}\")\n",
                "\n",
                "# plot only successful models\n",
                "if len(ok) == 0:\n",
                "    raise RuntimeError(\"No models ran successfully.\")\n",
                "\n",
                "fig, axes = plt.subplots(1, len(ok), figsize=(5 * len(ok), 5))\n",
                "if len(ok) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for ax, (name, sim_map) in zip(axes, ok):\n",
                "    ax.imshow(sim_map, cmap=\"jet\")\n",
                "    ax.set_title(f\"{name} - '{text_query}'\")\n",
                "    ax.axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3835beb",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "\n",
                "VALID_RESOLUTIONS_NARADIO = [\n",
                "    (192, 192), (224, 224), (256, 256), (288, 288), (320, 320), (384, 384),\n",
                "    (448, 448), (512, 512), (640, 640), (768, 768), (896, 896), (1024, 1024),\n",
                "    (256, 384), (384, 256), (336, 448), (448, 336), (512, 768), (768, 512),\n",
                "    (512, 896), (896, 512), (720, 1280), (800, 1280), (768, 1024), (1024, 768),\n",
                "]\n",
                "\n",
                "def is_naradio_like(enc):\n",
                "    return hasattr(getattr(enc, \"model\", None), \"naradio\") and hasattr(enc.model.naradio, \"input_resolution\")\n",
                "\n",
                "def pick_best_resolution(hw, candidates):\n",
                "    h, w = hw\n",
                "    ar = w / h\n",
                "    best = None\n",
                "    best_score = float(\"inf\")\n",
                "    for H, W in candidates:\n",
                "        ar2 = W / H\n",
                "        score = abs(math.log(ar2 / ar)) + 0.15 * abs(math.log((H * W) / (h * w)))\n",
                "        if score < best_score:\n",
                "            best_score = score\n",
                "            best = (H, W)\n",
                "    return best\n",
                "\n",
                "def resize_to(x, hw):\n",
                "    return F.interpolate(x, size=hw, mode=\"bilinear\", align_corners=False)\n",
                "\n",
                "def letterbox_to(x, target_hw):\n",
                "    B, C, H, W = x.shape\n",
                "    th, tw = target_hw\n",
                "    scale = min(tw / W, th / H)\n",
                "    nh = int(round(H * scale))\n",
                "    nw = int(round(W * scale))\n",
                "    x = F.interpolate(x, size=(nh, nw), mode=\"bilinear\", align_corners=False)\n",
                "\n",
                "    pad_h = th - nh\n",
                "    pad_w = tw - nw\n",
                "    top = pad_h // 2\n",
                "    bottom = pad_h - top\n",
                "    left = pad_w // 2\n",
                "    right = pad_w - left\n",
                "    return F.pad(x, (left, right, top, bottom), value=0.0)\n",
                "\n",
                "def ensure_bchw(feats):\n",
                "    if not isinstance(feats, torch.Tensor):\n",
                "        raise TypeError(f\"Model returned {type(feats)} not a Tensor\")\n",
                "    if feats.ndim != 4:\n",
                "        raise ValueError(f\"Expected (B,C,H,W), got {tuple(feats.shape)}\")\n",
                "    return feats\n",
                "\n",
                "def prepare_input_for_model(name, model, image_tensor):\n",
                "    lname = name.lower()\n",
                "\n",
                "    # SAM-based fusion models are broken right now (segmenter output mismatch)\n",
                "    # BUT we fixed it in the library, so we can try running them!\n",
                "    # if \"concept_fusion\" in lname or \"x_fusion\" in lname:\n",
                "    #    raise RuntimeError(\"SAM-based segmenter output mismatch; needs library patch. Skipping.\")\n",
                "\n",
                "    # DINO/VIT fusion: force 224x224 (14x14 tokens for patch=16)\n",
                "    if \"dino\" in lname:\n",
                "        return resize_to(image_tensor, (224, 224)), \"resize 224x224 (ViT 14x14 grid)\"\n",
                "\n",
                "    # NARadio: letterbox to a supported resolution\n",
                "    if is_naradio_like(model):\n",
                "        target_hw = pick_best_resolution(image_tensor.shape[-2:], VALID_RESOLUTIONS_NARADIO)\n",
                "        return letterbox_to(image_tensor, target_hw), f\"letterbox {target_hw}\"\n",
                "\n",
                "    # default: no change\n",
                "    return image_tensor, \"native\"\n",
                "\n",
                "# --- Run & plot ---\n",
                "text_query = \"chair\"\n",
                "ok = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"Running {name}...\")\n",
                "    try:\n",
                "        img_in, how = prepare_input_for_model(name, model, image_tensor)\n",
                "        print(f\"  -> {how}\")\n",
                "\n",
                "        with torch.no_grad():\n",
                "            feats = ensure_bchw(model(img_in))\n",
                "            txt = model.encode_text([text_query])\n",
                "\n",
                "            feats = F.normalize(feats, dim=1)\n",
                "            txt = F.normalize(txt, dim=1)\n",
                "            \n",
                "            # Ensure same dtype\n",
                "            txt = txt.to(dtype=feats.dtype)\n",
                "\n",
                "            sim = torch.einsum(\"bchw,bc->bhw\", feats, txt)[0].cpu().numpy()\n",
                "\n",
                "        ok.append((name, sim))\n",
                "    except Exception as e:\n",
                "        print(f\"  !! skipping {name}: {type(e).__name__}: {e}\")\n",
                "\n",
                "if not ok:\n",
                "    raise RuntimeError(\"No models ran successfully.\")\n",
                "\n",
                "fig, axes = plt.subplots(1, len(ok), figsize=(5 * len(ok), 5))\n",
                "if len(ok) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for ax, (name, sim_map) in zip(axes, ok):\n",
                "    ax.imshow(sim_map, cmap=\"jet\")\n",
                "    ax.set_title(f\"{name}\\n'{text_query}'\")\n",
                "    ax.axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bef808d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "VALID_RESOLUTIONS = [\n",
                "    (192, 192), (224, 224), (256, 256), (288, 288), (320, 320), (384, 384),\n",
                "    (448, 448), (512, 512), (640, 640), (768, 768), (896, 896), (1024, 1024),\n",
                "    (256, 384), (384, 256), (336, 448), (448, 336), (512, 768), (768, 512),\n",
                "    (512, 896), (896, 512), (720, 1280), (800, 1280), (768, 1024), (1024, 768),\n",
                "]\n",
                "\n",
                "def tensor_to_img(x):  # (1,3,H,W) -> (H,W,3) float [0,1]\n",
                "    x = x[0].detach().cpu().permute(1, 2, 0).numpy()\n",
                "    return np.clip(x, 0.0, 1.0)\n",
                "\n",
                "def normalize_map(m):\n",
                "    m = m.astype(np.float32)\n",
                "    m = m - m.min()\n",
                "    return m / (m.max() + 1e-8)\n",
                "\n",
                "def letterbox_to(x, target_hw):\n",
                "    # x: (B,C,H,W), target_hw: (H,W)\n",
                "    B, C, H, W = x.shape\n",
                "    th, tw = target_hw\n",
                "\n",
                "    scale = min(tw / W, th / H)\n",
                "    nh = int(round(H * scale))\n",
                "    nw = int(round(W * scale))\n",
                "\n",
                "    x = F.interpolate(x, size=(nh, nw), mode=\"bilinear\", align_corners=False)\n",
                "\n",
                "    pad_h = th - nh\n",
                "    pad_w = tw - nw\n",
                "    top = pad_h // 2\n",
                "    bottom = pad_h - top\n",
                "    left = pad_w // 2\n",
                "    right = pad_w - left\n",
                "\n",
                "    return F.pad(x, (left, right, top, bottom), value=0.0)\n",
                "\n",
                "def run_all_resolutions_for_model(model, image_tensor, text_query, resolutions, device=None):\n",
                "    if device is None:\n",
                "        device = image_tensor.device\n",
                "\n",
                "    # encode text once\n",
                "    with torch.no_grad():\n",
                "        text_emb = model.encode_text([text_query]).to(device)\n",
                "        text_emb = F.normalize(text_emb, dim=1)\n",
                "\n",
                "    results = []\n",
                "    for hw in resolutions:\n",
                "        img_in = letterbox_to(image_tensor, hw)\n",
                "\n",
                "        with torch.no_grad():\n",
                "            feats = model(img_in)              # (1,C,H,W)\n",
                "            feats = F.normalize(feats, dim=1)\n",
                "\n",
                "            sim = torch.einsum(\"bchw,bc->bhw\", feats, text_emb)  # (1,H,W)\n",
                "            sim_map = sim[0].detach().cpu().numpy()\n",
                "            sim_norm = normalize_map(sim_map)\n",
                "\n",
                "        results.append((hw, img_in.detach().cpu(), sim_norm))\n",
                "\n",
                "        # keep GPU memory sane\n",
                "        del img_in, feats, sim\n",
                "\n",
                "    return results\n",
                "\n",
                "def plot_resolution_grid(model_name, image_tensor, text_query, results, max_cols=4):\n",
                "    orig_img = tensor_to_img(image_tensor)\n",
                "\n",
                "    # We will plot: 1 \"Original\" + for each res: (heatmap, overlay)\n",
                "    n_tiles = 1 + 2 * len(results)\n",
                "    ncols = max_cols\n",
                "    nrows = math.ceil(n_tiles / ncols)\n",
                "\n",
                "    fig = plt.figure(figsize=(4.2 * ncols, 3.6 * nrows))\n",
                "    fig.suptitle(f\"{model_name} \u2014 all resolutions \u2014 query='{text_query}'\", fontsize=14)\n",
                "\n",
                "    def add_ax(idx):\n",
                "        ax = fig.add_subplot(nrows, ncols, idx + 1)\n",
                "        ax.axis(\"off\")\n",
                "        return ax\n",
                "\n",
                "    k = 0\n",
                "\n",
                "    # (0) Original\n",
                "    ax = add_ax(k); k += 1\n",
                "    ax.imshow(orig_img)\n",
                "    ax.set_title(\"Original\", fontsize=10)\n",
                "\n",
                "    # Each resolution: heatmap + overlay (on the exact letterboxed input)\n",
                "    for (H, W), img_in_cpu, sim_norm in results:\n",
                "        in_img = tensor_to_img(img_in_cpu)\n",
                "\n",
                "        ax = add_ax(k); k += 1\n",
                "        ax.imshow(sim_norm, cmap=\"jet\")\n",
                "        ax.set_title(f\"Heatmap {H}\u00d7{W}\", fontsize=10)\n",
                "\n",
                "        ax = add_ax(k); k += 1\n",
                "        ax.imshow(in_img)\n",
                "        ax.imshow(sim_norm, cmap=\"jet\", alpha=0.45)\n",
                "        ax.set_title(f\"Overlay {H}\u00d7{W}\", fontsize=10)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# ---- run for all models ----\n",
                "text_query = \"chair\"\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n=== {name} ===\")\n",
                "    results = run_all_resolutions_for_model(\n",
                "        model=model,\n",
                "        image_tensor=image_tensor,\n",
                "        text_query=text_query,\n",
                "        resolutions=VALID_RESOLUTIONS,\n",
                "    )\n",
                "    plot_resolution_grid(name, image_tensor, text_query, results, max_cols=4)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d7fabc5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# ---- if your image_tensor is ImageNet-normalized, set these; else set to None ----\n",
                "IMAGENET_MEAN = None  # (0.485, 0.456, 0.406)\n",
                "IMAGENET_STD  = None  # (0.229, 0.224, 0.225)\n",
                "\n",
                "def denorm_if_needed(x):\n",
                "    # x: (1,3,H,W)\n",
                "    if IMAGENET_MEAN is None or IMAGENET_STD is None:\n",
                "        return x\n",
                "    mean = torch.tensor(IMAGENET_MEAN, device=x.device).view(1,3,1,1)\n",
                "    std  = torch.tensor(IMAGENET_STD,  device=x.device).view(1,3,1,1)\n",
                "    return x * std + mean\n",
                "\n",
                "def to_float01_rgb(x):\n",
                "    # ensure model input is float in [0,1]\n",
                "    x = denorm_if_needed(x)\n",
                "    return x.clamp(0, 1)\n",
                "\n",
                "def tensor_to_img01(x):  # (1,3,H,W) -> (H,W,3)\n",
                "    x = x[0].detach().cpu().permute(1,2,0).numpy()\n",
                "    return np.clip(x, 0.0, 1.0)\n",
                "\n",
                "def letterbox_with_mask(x, target_hw):\n",
                "    # returns padded image and a mask of valid (non-pad) pixels\n",
                "    B, C, H, W = x.shape\n",
                "    th, tw = target_hw\n",
                "    scale = min(tw / W, th / H)\n",
                "    nh = int(round(H * scale))\n",
                "    nw = int(round(W * scale))\n",
                "\n",
                "    x_rs = F.interpolate(x, size=(nh, nw), mode=\"bilinear\", align_corners=False)\n",
                "\n",
                "    pad_h = th - nh\n",
                "    pad_w = tw - nw\n",
                "    top = pad_h // 2\n",
                "    bottom = pad_h - top\n",
                "    left = pad_w // 2\n",
                "    right = pad_w - left\n",
                "\n",
                "    x_pad = F.pad(x_rs, (left, right, top, bottom), value=0.0)\n",
                "\n",
                "    mask = torch.zeros((1, 1, th, tw), device=x.device, dtype=x.dtype)\n",
                "    mask[:, :, top:top+nh, left:left+nw] = 1.0\n",
                "    return x_pad, mask\n",
                "\n",
                "def robust_norm(sim, mask=None, lo=5, hi=95):\n",
                "    # sim: (H,W) numpy, mask: (H,W) numpy {0,1} optional\n",
                "    if mask is not None:\n",
                "        vals = sim[mask > 0.5]\n",
                "    else:\n",
                "        vals = sim.reshape(-1)\n",
                "    if vals.size == 0:\n",
                "        return np.zeros_like(sim, dtype=np.float32), (0.0, 1.0)\n",
                "\n",
                "    vmin = np.percentile(vals, lo)\n",
                "    vmax = np.percentile(vals, hi)\n",
                "    if vmax <= vmin + 1e-8:\n",
                "        vmax = vmin + 1e-8\n",
                "    simn = (sim - vmin) / (vmax - vmin)\n",
                "    return np.clip(simn, 0, 1).astype(np.float32), (float(vmin), float(vmax))\n",
                "\n",
                "def overlay(ax, base_img01, heat01, alpha=0.45):\n",
                "    ax.imshow(base_img01)\n",
                "    ax.imshow(heat01, cmap=\"jet\", alpha=alpha, interpolation=\"nearest\")\n",
                "\n",
                "# ----------------- main comparison -----------------\n",
                "\n",
                "def run_all_resolutions_compare(model, image_tensor, text_query, resolutions):\n",
                "    device = image_tensor.device\n",
                "    img01 = to_float01_rgb(image_tensor)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        text_emb = model.encode_text([text_query]).to(device)\n",
                "        text_emb = F.normalize(text_emb, dim=1)\n",
                "\n",
                "    # first pass: collect raw sims + masks so we can normalize consistently per model\n",
                "    sims = []\n",
                "    masks = []\n",
                "    imgs_in = []\n",
                "\n",
                "    for (H, W) in resolutions:\n",
                "        img_in, mask = letterbox_with_mask(img01, (H, W))\n",
                "\n",
                "        with torch.no_grad():\n",
                "            feats = model(img_in)\n",
                "            feats = F.normalize(feats, dim=1)\n",
                "            sim = torch.einsum(\"bchw,bc->bhw\", feats, text_emb)[0]  # (H,W)\n",
                "        sims.append(sim.detach().cpu().numpy())\n",
                "        masks.append(mask[0,0].detach().cpu().numpy())\n",
                "        imgs_in.append(img_in.detach().cpu())\n",
                "\n",
                "        del img_in, mask, feats, sim\n",
                "\n",
                "    # compute ONE normalization range for the whole model across all resolutions (excluding padding)\n",
                "    all_vals = np.concatenate([s[m > 0.5].ravel() for s, m in zip(sims, masks) if (m > 0.5).any()])\n",
                "    vmin = np.percentile(all_vals, 5)\n",
                "    vmax = np.percentile(all_vals, 95)\n",
                "    if vmax <= vmin + 1e-8:\n",
                "        vmax = vmin + 1e-8\n",
                "\n",
                "    # plot grid: Original + for each res: Heatmap, Overlay\n",
                "    n_tiles = 1 + 2 * len(resolutions)\n",
                "    ncols = 4\n",
                "    nrows = int(np.ceil(n_tiles / ncols))\n",
                "    fig = plt.figure(figsize=(4.2*ncols, 3.6*nrows))\n",
                "    fig.suptitle(f\"{text_query} \u2014 {getattr(model, 'name', 'model')} (consistent norm, padding masked)\", fontsize=14)\n",
                "\n",
                "    k = 1\n",
                "    ax = fig.add_subplot(nrows, ncols, k); k += 1\n",
                "    ax.axis(\"off\")\n",
                "    ax.imshow(tensor_to_img01(img01))\n",
                "    ax.set_title(\"Original\", fontsize=10)\n",
                "\n",
                "    for (H, W), sim_np, mask_np, img_in_cpu in zip(resolutions, sims, masks, imgs_in):\n",
                "        simn = np.clip((sim_np - vmin) / (vmax - vmin), 0, 1).astype(np.float32)\n",
                "        simn_masked = simn * mask_np  # zero out padding visually too\n",
                "\n",
                "        ax = fig.add_subplot(nrows, ncols, k); k += 1\n",
                "        ax.axis(\"off\")\n",
                "        ax.imshow(simn_masked, cmap=\"jet\", interpolation=\"nearest\")\n",
                "        ax.set_title(f\"Heat {H}\u00d7{W}\", fontsize=10)\n",
                "\n",
                "        ax = fig.add_subplot(nrows, ncols, k); k += 1\n",
                "        ax.axis(\"off\")\n",
                "        base = tensor_to_img01(img_in_cpu)\n",
                "        overlay(ax, base, simn_masked, alpha=0.45)\n",
                "        ax.set_title(f\"Overlay {H}\u00d7{W}\", fontsize=10)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Example:\n",
                "run_all_resolutions_compare(models[\"naradio_fusion\"], image_tensor, \"screen\", VALID_RESOLUTIONS)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ad2de7f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}