{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Embed SLAM Models Comparison\n",
                "\n",
                "This notebook compares the models from `embed_slam`: ConceptFusion, DINOFusion, XFusion, and NARadioFusion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "from vlm_eval.core import EncoderRegistry\n",
                "from vlm_eval.encoders import *\n",
                "\n",
                "# Ensure models are registered\n",
                "print(\"Available encoders:\", EncoderRegistry.list_available())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Models\n",
                "# Note: You need to have the necessary checkpoints and dependencies installed.\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "models = {}\n",
                "model_names = [\"concept_fusion\", \"dino_fusion\", \"x_fusion\", \"naradio_fusion\"]\n",
                "\n",
                "for name in model_names:\n",
                "    try:\n",
                "        print(f\"Loading {name}...\")\n",
                "        models[name] = EncoderRegistry.get(name, device=device)\n",
                "        print(f\"Loaded {name}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {name}: {e}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a sample image\n",
                "# Replace with your image path\n",
                "image_path = \"../examples/sample_image.jpg\"\n",
                "\n",
                "if not os.path.exists(image_path):\n",
                "    # Create a dummy image if not exists\n",
                "    dummy_img = Image.fromarray(np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8))\n",
                "    dummy_img.save(image_path)\n",
                "    print(f\"Created dummy image at {image_path}\")\n",
                "\n",
                "image = Image.open(image_path).convert(\"RGB\")\n",
                "plt.imshow(image)\n",
                "plt.title(\"Input Image\")\n",
                "plt.show()\n",
                "\n",
                "# Preprocess\n",
                "image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
                "image_tensor = image_tensor.unsqueeze(0).to(device)\n",
                "print(\"Image tensor shape:\", image_tensor.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Inference and Compare\n",
                "text_query = \"chair\"\n",
                "\n",
                "fig, axes = plt.subplots(1, len(models), figsize=(20, 5))\n",
                "if len(models) == 1:\n",
                "    axes = [axes]\n",
                "\n",
                "for i, (name, model) in enumerate(models.items()):\n",
                "    print(f\"Running {name}...\")\n",
                "    with torch.no_grad():\n",
                "        # Get image features\n",
                "        features = model(image_tensor) # (B, C, H, W)\n",
                "        \n",
                "        # Get text features\n",
                "        text_emb = model.encode_text([text_query]) # (1, C)\n",
                "        \n",
                "        # Compute similarity\n",
                "        # features: (1, C, H, W)\n",
                "        # text_emb: (1, C)\n",
                "        sim = torch.einsum(\"bchw,bc->bhw\", features, text_emb)\n",
                "        \n",
                "        sim_map = sim[0].cpu().numpy()\n",
                "        \n",
                "        axes[i].imshow(sim_map, cmap=\"jet\")\n",
                "        axes[i].set_title(f\"{name} - '{text_query}'\")\n",
                "        axes[i].axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}